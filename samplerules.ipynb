{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13657703,"sourceType":"datasetVersion","datasetId":8683141}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport shutil\n\n# Source and destination paths\nsource_path = \"/kaggle/input/rulesdb/rules.db\"\ndestination_path = \"/kaggle/working/rules.db\"\n\n# Copy file\nshutil.copy2(source_path, destination_path)\n\nprint(f\"ðŸ“‚ File copied from {source_path} â†’ {destination_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize LLM\n!pip install -q transformers accelerate bitsandbytes sentencepiece\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# âœ… Model and tokenizer\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# âœ… Load model in 4-bit with CPU offload for T4 / smaller GPUs\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",                     # auto-assign layers to GPU/CPU\n    load_in_4bit=True,                     # 4-bit quantization\n    llm_int8_enable_fp32_cpu_offload=False, # offload layers to CPU if needed\n)\nmodel.eval()\n\n# âœ… Prepare prompt\nprompt = \"Explain Machine Learning in a nutshell.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n# âœ… Generate text\nwith torch.inference_mode():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=128,\n        temperature=0.7,\n        top_p=0.9,\n        do_sample=False\n    )\n\n# âœ… Decode output\nresult = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"\\n=== Mistral-7B-Instruct Output (4-bit) ===\\n\")\nprint(result)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Populate top_rules table Business Description by invoking LLM\nimport sqlite3\nimport json\n\n# Connect to the database\nconn = sqlite3.connect('/kaggle/working/rules.db')\ncursor = conn.cursor()\n\n# Fetch all records from rule_json_store\ncursor.execute(\"SELECT key, rule_type, rulejson FROM top_rules where businessdescription is null and LENGTH(rulejson) < 100000 LIMIT 3000\")\nrows = cursor.fetchall()\n\nprint(f\"ðŸ” Found {len(rows)} records to process.\\n\")\n\nfor i, (key, rule_type, rulejson) in enumerate(rows, 1):\n    try:\n        # Load existing JSON\n        # data = json.loads(rulejson)\n        # print(f\"ðŸ”  {rulejson} \\n\")\n        # ðŸ§© Example modification (you can customize this logic):\n        # Add or update a \"lastUpdated\" field\n        # data[\"lastUpdated\"] = \"2025-10-31\"\n\n        # Optional: apply transformations, cleanup, re-extraction, etc.\n        # e.g., data = extract(data, selector_json)\n\n        # Convert back to string\n        # updated_json = json.dumps(data, ensure_ascii=False, indent=2)\n\n        prompt = f\"Given the '{rulejson}' Pega '{rule_type}' rule JSON, summarize the rule in plain English. Explain what the rule does and when the rule triggers. Avoid technical JSON details unless necessary, and make the description readable for a business user.\"\n        # print(f\"âš ï¸ prompt: {prompt}\")\n\n        # Generate text\n        # Tokenize input\n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n\n        # Generate text\n        with torch.inference_mode():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=1024,\n                temperature=0.1,\n                top_p=0.9,\n                do_sample=True\n            )\n\n        # Extract only the new tokens (excluding the input)\n        generated_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n        result = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n\n\n        # Update the record\n        cursor.execute(\"\"\"\n            UPDATE top_rules\n            SET businessdescription = ?\n            WHERE key = ?\n        \"\"\", (result, key))\n        # print(f\"âš ï¸ response: {result}\")\n        conn.commit()\n        print(f\"âœ… Updated ({i}/{len(rows)}) pzInsKey: {key}\")\n\n    except json.JSONDecodeError:\n        print(f\"âš ï¸ Skipped invalid JSON for pzInsKey: {key}\")\n        continue\n\n# Commit all updates\n# conn.commit()\nconn.close()\n\nprint(\"\\nðŸŽ‰ All rulejson entries have been updated successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T18:31:01.157042Z","iopub.execute_input":"2025-11-05T18:31:01.157357Z","iopub.status.idle":"2025-11-05T18:31:01.278083Z","shell.execute_reply.started":"2025-11-05T18:31:01.157331Z","shell.execute_reply":"2025-11-05T18:31:01.276947Z"}},"outputs":[],"execution_count":null}]}