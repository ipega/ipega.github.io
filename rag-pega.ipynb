{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13663169,"sourceType":"datasetVersion","datasetId":8686876},{"sourceId":13693454,"sourceType":"datasetVersion","datasetId":8709753}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install protobuf==4.25.3 --force-reinstall\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T11:12:34.730832Z","iopub.execute_input":"2025-11-09T11:12:34.731554Z","iopub.status.idle":"2025-11-09T11:12:39.452305Z","shell.execute_reply.started":"2025-11-09T11:12:34.731512Z","shell.execute_reply":"2025-11-09T11:12:39.451388Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting protobuf==4.25.3\n  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\nDownloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 6.33.0\n    Uninstalling protobuf-6.33.0:\n      Successfully uninstalled protobuf-6.33.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.38.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.25.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-4.25.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# %% [markdown]\n# # üß† Pega Rules ‚Üí SQLite ‚Üí Vector Database (Chroma)\n# \n# This notebook reads your Pega rule data from SQLite, converts it into\n# embeddings, stores it in a Chroma vector database, and allows semantic search.\n# \n# Optional next step: integrate Mistral (4-bit) for reasoning.\n\n# %%\n# 1Ô∏è‚É£ Install dependencies\n!pip install -q chromadb sentence-transformers sqlite-utils\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T11:14:07.579623Z","iopub.execute_input":"2025-11-09T11:14:07.580048Z","iopub.status.idle":"2025-11-09T11:14:12.241565Z","shell.execute_reply.started":"2025-11-09T11:14:07.579993Z","shell.execute_reply":"2025-11-09T11:14:12.240565Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\n# %%\n# 2Ô∏è‚É£ Imports\nimport sqlite3\nimport json\nfrom chromadb import Client\nfrom sentence_transformers import SentenceTransformer\n\n# %%\n# 3Ô∏è‚É£ Setup: database + embedding model\ndb_path = \"/kaggle/input/rulesdb/rules.db\"  # üîÅ update this\n\nconn = sqlite3.connect(db_path)\ncursor = conn.cursor()\n\nembedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Initialize Chroma vector DB\nchroma = Client()\ncollection = chroma.get_or_create_collection(\"pega_rules_db\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T11:14:18.495903Z","iopub.execute_input":"2025-11-09T11:14:18.496302Z","iopub.status.idle":"2025-11-09T11:14:19.327558Z","shell.execute_reply.started":"2025-11-09T11:14:18.496266Z","shell.execute_reply":"2025-11-09T11:14:19.326433Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\n# %%\n# 4Ô∏è‚É£ Helper function to convert each rule row into meaningful text\ndef format_rule_text(row):\n    \"\"\"\n    Convert DB row to descriptive text for semantic embedding.\n    row order: (id, key, rule_type, rule_class, name, description, rulejson, businessdescription)\n    \"\"\"\n    try:\n        rule_json = json.loads(row[6]) if row[6] else {}\n    except Exception:\n        rule_json = {}\n\n    return f\"\"\"\nRule Key: {row[1]}\nRule Type: {row[2]}\nRule Class: {row[3]}\nRule Name: {row[4]}\nDescription: {row[5]}\nBusiness Description: {row[7]}\nRule JSON Summary: {json.dumps(rule_json, indent=2)[:1000]}\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T11:14:57.865077Z","iopub.execute_input":"2025-11-09T11:14:57.865690Z","iopub.status.idle":"2025-11-09T11:14:57.871032Z","shell.execute_reply.started":"2025-11-09T11:14:57.865664Z","shell.execute_reply":"2025-11-09T11:14:57.870224Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\n# %%\n# 5Ô∏è‚É£ Fetch rules and insert into the vector DB\ncursor.execute(\"SELECT * FROM top_rules\")\nrows = cursor.fetchall()\n\nfor row in rows:\n    rule_id = str(row[0])\n    text = format_rule_text(row)\n    embedding = embedder.encode(text).tolist()\n\n    collection.add(\n        ids=[rule_id],\n        embeddings=[embedding],\n        metadatas=[{\n            \"key\": row[1],\n            \"rule_type\": row[2],\n            \"rule_class\": row[3],\n            \"name\": row[4]\n        }],\n        documents=[text]\n    )\n\nprint(f\"‚úÖ Loaded {len(rows)} rules into vector database.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T11:19:53.892634Z","iopub.execute_input":"2025-11-09T11:19:53.893035Z","iopub.status.idle":"2025-11-09T11:58:04.500779Z","shell.execute_reply.started":"2025-11-09T11:19:53.893008Z","shell.execute_reply":"2025-11-09T11:58:04.499381Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Loaded 20266 rules into vector database.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"\n# %%\n# 6Ô∏è‚É£ Example query: find relevant rules for a user story\nuser_story = \"We need to validate email address in the customer registration form.\"\n\nquery_embedding = embedder.encode(user_story).tolist()\nresults = collection.query(query_embeddings=[query_embedding], n_results=5)\n\nprint(\"\\nüîç Top Matching Rules:\\n\")\nfor doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n    print(f\"{meta['name']} ({meta['rule_type']}) - {meta['rule_class']}\")\n    print(doc[:400], \"\\n---\\n\")\n\n\n\n# %%\n# ‚úÖ Summary\n# You now have:\n# - Pega rules indexed from SQLite\n# - A working vector database for semantic retrieval\n# - Ready integration point for a reasoning model like Mistral 4-bit\n\n# Next step (optional):\n# - Integrate Mistral to interpret user stories and explain rule changes.\n# Would you like me to add that part next?\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T11:19:37.002097Z","iopub.execute_input":"2025-11-09T11:19:37.002513Z","iopub.status.idle":"2025-11-09T11:19:37.031803Z","shell.execute_reply.started":"2025-11-09T11:19:37.002484Z","shell.execute_reply":"2025-11-09T11:19:37.030876Z"}},"outputs":[{"name":"stdout","text":"\nüîç Top Matching Rules:\n\nValUAUserInformation (Rule-Obj-Model) - Code-Security\n\nRule Key: RULE-OBJ-MODEL CODE-SECURITY VALUAUSERINFORMATION #20190426T065753.023 GMT\nRule Type: Rule-Obj-Model\nRule Class: Code-Security\nRule Name: ValUAUserInformation\nDescription: validates the unauthentication user information.\nBusiness Description: \n\nThis rule retrieves the operator information based on the provided UserIdentifier. If the UserIdentifier is empty, it sets the status message to \n---\n\nPopulateFromEmailStringHeaderExt (Rule-Obj-Model) - Data-EmailAccount\n\nRule Key: RULE-OBJ-MODEL DATA-EMAILACCOUNT POPULATEFROMEMAILSTRINGHEADEREXT #20230306T084433.108 GMT\nRule Type: Rule-Obj-Model\nRule Class: Data-EmailAccount\nRule Name: PopulateFromEmailStringHeaderExt\nDescription: Impl teams who wants to send the email on users email instead of system account email address\nBusiness Description: \n\nThis rule is designed to set the email address of a superclass obje \n---\n\nGetEmailAccountName (Rule-Obj-Model) - PegaFW-Document-Email\n\nRule Key: RULE-OBJ-MODEL PEGAFW-DOCUMENT-EMAIL GETEMAILACCOUNTNAME #20230306T084433.553 GMT\nRule Type: Rule-Obj-Model\nRule Class: PegaFW-Document-Email\nRule Name: GetEmailAccountName\nDescription: \nBusiness Description: \n\nThis rule, when a business user provides an email account name, sets the email account name in the \"Default\" account using the \"SET\" action. The rule triggers whenever a business \n---\n\nPrepareEmailHeadersExt (Rule-Obj-Model) - PegaFW-Document-Email\n\nRule Key: RULE-OBJ-MODEL PEGAFW-DOCUMENT-EMAIL PREPAREEMAILHEADERSEXT #20230306T084433.733 GMT\nRule Type: Rule-Obj-Model\nRule Class: PegaFW-Document-Email\nRule Name: PrepareEmailHeadersExt\nDescription: Customer want to send mail from the logged in operator this dt needs to be overridden there layer\nBusiness Description: \n\nThis rule, when enabled, extends the email sending process for specific imp \n---\n\nGetItemAddInRequest (Rule-Obj-Model) - PegaFW-Int-EWS-ExchangeService-\n\nRule Key: RULE-OBJ-MODEL PEGAFW-INT-EWS-EXCHANGESERVICE- GETITEMADDINREQUEST #20230306T084435.195 GMT\nRule Type: Rule-Obj-Model\nRule Class: PegaFW-Int-EWS-ExchangeService-\nRule Name: GetItemAddInRequest\nDescription: \nBusiness Description: \n\nThis rule updates the page \"PrimaryPageName\" with the Appointment object and sets the Exchange version and impersonation header. It also sets the shape of the \n---\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# %%\n# 7Ô∏è‚É£ (Optional) Persist the Chroma DB for reuse\ncollection.persist()\nprint(\"üíæ Vector database saved successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}